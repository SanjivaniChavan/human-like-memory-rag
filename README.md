# ğŸ§  Human-Like Memory System for Conversational AI

This project implements a **human-inspired memory architecture** for AI agents, combining:

- ğŸ§  **Episodic Memory** â€” raw chronological experiences  
- ğŸ§  **Semantic Memory** â€” abstracted long-term knowledge  
- ğŸ” **FAISS Vector Search** â€” scalable similarity retrieval  
- âš–ï¸ **Weighted Recall** â€” relevance + recency + importance  
- ğŸ“ **Summarization Pipeline** â€” consolidates long-term memory  
- ğŸš€ **FastAPI Server** â€” memory as an API for LLM agents  

This system enables AI agents to **remember like humans**, supporting long-term personality, contextual understanding, and emotional continuity.

---

## ğŸ§  1. Why This Memory System?

LLMs are stateless â€” without memory, they forget everything.

This causes:

- âŒ No long-term personalization  
- âŒ Repetition or contradictions  
- âŒ High token costs  
- âŒ No evolving memory  

This project solves these issues using a **dual-memory cognitive architecture**.

---

## ğŸ§© 2. Architecture Overview

```mermaid
flowchart TD
    U[User Query] --> E[Embed Query]
    E --> R[FAISS Retriever]
    R --> W[Weighted Scoring: Relevance + Recency + Importance]
    W --> M[Top-K Memories]
    M --> S[Summarizer]
    S --> C[Context for LLM]
```

---

## âš™ï¸ 3. Key Features

### ğŸ”¹ Dual Memory System

| Memory Type       | Purpose                                                 |
|------------------|----------------------------------------------------------|
| **Episodic Memory**  | Stores raw, time-ordered interactions                    |
| **Semantic Memory**  | Stores condensed summaries & stable long-term knowledge  |

---

### ğŸ”¹ FAISS-Based Vector Retrieval

- Fast similarity search  
- Scales to 100K+ memories  
- CPU-friendly retrieval  

---

### ğŸ”¹ Weighted Recall Formula

```text
score = Î± * relevance
      + Î² * recency
      + Î³ * importance
```

This produces **human-like recall behavior**.

---

### ğŸ”¹ Summarization Layer

- Compresses memory  
- Removes noise  
- Supports long-term continuity  

---

### ğŸ”¹ FastAPI Backend

Two main endpoints:

```
POST /remember
POST /recall
```

---

## ğŸ— 4. Project Structure

```
src/
â”‚â”€â”€ api.py               # FastAPI memory API
â”‚â”€â”€ config.py            # Hyperparameters + scoring
â”‚â”€â”€ embeddings.py        # Embedding model loader
â”‚â”€â”€ memory_store.py      # Dual-memory architecture
â”‚â”€â”€ summarizer.py        # Summarization logic
â”‚â”€â”€ __init__.py
```

---

## ğŸ§ª 5. API Usage

### â¤ Store a memory

```json
POST /remember
{
  "text": "User said they love AI research.",
  "importance": 0.8,
  "memory_type": "episodic"
}
```

---

### â¤ Recall memories

```json
POST /recall
{
  "query": "What does the user like?",
  "k": 5
}
```

---

### Example Response

```json
{
  "memories": [
    {
      "id": "episodic-0",
      "text": "User said they love AI research.",
      "score": 0.94
    }
  ],
  "summary": "The user loves AI research."
}
```

---

## ğŸš€ 6. Run Locally

### Install dependencies

```bash
pip install -r requirements.txt
```

### Start server

```bash
uvicorn src.api:app --reload
```

Open Swagger docs:  
ğŸ‘‰ http://127.0.0.1:8000/docs

---

## ğŸ“ˆ 7. Performance & Scalability

- âš¡ Sub-millisecond retrieval  
- âš¡ Embedding caching  
- âš¡ Supports 100K+ memories  
- âš¡ Time-decay recency scoring  

Ideal for:

- LLM agents  
- RAG memory systems  
- Personalized assistants  
- Character AI  

---

## ğŸ”® 8. Future Enhancements

- Forgetting mechanisms  
- Emotional weighting  
- Semantic consolidation with LLM  
- Multi-user memory isolation  
- Dashboard for visualizing memory graphs  

---

## ğŸ‘©â€ğŸ’» Author

**Sanjivani Chavan**  
AI Engineer | LLM Systems | Memory Architectures | RAG Engineering

